{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c5a677-711e-4496-8409-89405715f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a42b69a-f8aa-475e-94df-7fdefe68b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种\\\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73ed390-bd22-43cf-a4ac-5b89b71438d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e3dd96-f3b5-4231-a52b-7b0d5e43803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../datasets/train_set.csv', sep='\\t')\n",
    "test_df = pd.read_csv('../datasets/test_a.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a8ff9a-8fe8-4430-80d4-7097d544ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1149\n",
      "2416\n",
      "6336\n"
     ]
    }
   ],
   "source": [
    "# 加载word2vec字典\n",
    "load_vocal = joblib.load('../data/vocab/vocab_word2vec.pkl')\n",
    "\n",
    "print(load_vocal.get_stoi().get('349', 0))\n",
    "print(load_vocal.get_stoi().get('3113', 0))\n",
    "print(load_vocal.get_stoi().get('4806', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba006549-5a3e-4cec-b761-6f700cdeccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_truncate_pad(string,\n",
    "                       num_steps,  # 句子最大长度\n",
    "                       stoi,  # Dictionary mapping tokens to indices.\n",
    "                       padding_index):  # 填充字符'<pad>'在词典中的索引\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    # 获取字在Vocab对象中的位置\n",
    "    line = [stoi.get(word, 0) for word in string.split()]\n",
    "    # num_steps_half = int(num_steps / 2)\n",
    "    # if len(line) > num_steps:\n",
    "    #      return line[:num_steps_half] + line[-num_steps_half:]  # 句子截断(这里取前max_len/2和后max_len/2个)\n",
    "    if len(line) > num_steps:\n",
    "        # 直接返回列表速度较快\n",
    "        return line[:num_steps]  # 句子截断(这里取前max_len个)\n",
    "    return line + [padding_index] * (num_steps - len(line))  # 句子填充\n",
    "\n",
    "\n",
    "max_len = 6000\n",
    "X_train_data = train_df['text'].apply(split_truncate_pad, num_steps=max_len,\n",
    "                                      stoi=load_vocal.get_stoi(), padding_index=1)  # 这里设置句子最大长度为max_len\n",
    "X_test_data = test_df['text'].apply(split_truncate_pad, num_steps=max_len, stoi=load_vocal.get_stoi(),\n",
    "                                    padding_index=1)\n",
    "\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "105f28bc-efdc-42c0-900a-4eabd38117b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  28,  820,  358,  ...,    1,    1,    1],\n",
      "        [ 144, 1643,   71,  ...,    1,    1,    1],\n",
      "        [ 301,  691,   61,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [ 281,  795,  359,  ...,    1,    1,    1],\n",
      "        [1254,  348,  630,  ...,    1,    1,    1],\n",
      "        [  15,  133,  371,  ...,    1,    1,    1]]), tensor([ 0,  0,  8,  7,  1,  3,  7,  0,  1,  7,  5,  2,  2,  0,  2,  3,  2,  4,\n",
      "         1,  2,  2,  5,  0,  4,  0,  0,  0,  1,  0,  0,  2,  7,  8,  2,  1,  3,\n",
      "         4,  1,  1,  1, 11,  6,  0,  1,  7,  2,  5,  6,  3,  3,  4,  1,  6,  2,\n",
      "         0,  2,  5,  0,  0,  2, 11,  4,  8,  0,  1,  4,  5,  6,  1,  1,  2,  0,\n",
      "         4,  1,  4, 10,  4,  5,  4,  4,  2,  0,  5,  1,  2,  0,  5,  5,  8,  1,\n",
      "         3,  2,  0,  3,  5,  4,  1,  0,  3,  1,  7,  2,  3,  1,  3,  7,  1, 10,\n",
      "         1, 10,  3,  2,  7, 10,  1,  4,  2,  1,  0,  0,  5,  1,  0,  2,  2,  8,\n",
      "         5,  1,  1,  3,  0,  1, 11,  2,  3,  2,  8,  3,  6,  0,  1,  0,  3,  0,\n",
      "         8,  0, 10,  0,  2,  3,  1,  5,  1,  2,  0,  2,  2, 10,  8,  4,  2,  2,\n",
      "         5, 10,  0,  1,  2,  2,  0,  1,  8,  8,  3,  0,  2,  9,  0,  7,  9,  7,\n",
      "         1,  0,  2,  7,  4,  0,  3,  0,  4,  1,  1,  1,  2,  6,  1,  5,  0,  1,\n",
      "         4,  6,  8,  3,  4,  5,  2,  2,  0,  1,  3,  3,  1,  1,  1,  3,  0,  2,\n",
      "         2,  4,  1,  1,  1,  0,  2,  2, 10,  0, 11,  3,  0,  0,  2,  4,  2,  3,\n",
      "         4, 10,  4,  3,  1,  4,  2,  3,  3, 11,  1,  2,  6,  8, 10,  2,  6,  4,\n",
      "         6, 10,  1,  1])]\n",
      "torch.Size([256, 6000])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "dataset_tr = Data.TensorDataset(torch.tensor(X_train_data.values.tolist()), torch.tensor(y_train))\n",
    "dataloader_tr = Data.DataLoader(dataset_tr, 256, shuffle=True)\n",
    "\n",
    "for i in dataloader_tr:\n",
    "    print(i)\n",
    "    print(i[0].shape)  # i[0].shape=[batch_size, num_steps]\n",
    "    print(i[1].shape)  # i[1].shape=[batch_size, ]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f557f5b2-3031-4651-bbc8-6a77be8ecf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8654,  1.0556, -1.1478,  ...,  2.9752, -1.3487, -1.1243],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预训练词向量文件\n",
    "vector = torchtext.vocab.Vectors(name=\"cnew_300.txt\",\n",
    "                                 cache='word2vec')\n",
    "\n",
    "pretrained_vector = vector.get_vecs_by_tokens(load_vocal.get_itos())\n",
    "pretrained_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eeb8cd2-91d2-4ea0-b9f4-83326cc8ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN模型的pytorch实现(具体任务对应修改)\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    vocab_size : int\n",
    "        单词表的单词数目\n",
    "    embed_size : int\n",
    "        输出词向量的维度大小\n",
    "    kernel_sizes : tuple\n",
    "        一般来说:不同大小卷积核的组合通常优于同样大小的卷积核\n",
    "        不同卷积层卷积核的宽度;如:kernel_sizes=(3, 4, 5)\n",
    "    num_channels : tuple\n",
    "        不同卷积层输出通道数;如:num_channels=(100, 100, 100)\n",
    "    dropput_ratio : float\n",
    "        dropout层p值\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels, dropout_ratio=0.5):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        # 预训练的词嵌入层\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 14)  # 多分类\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # 通过nn.ModuleList()创建多个⼀维卷积层\n",
    "        self.convs = nn.ModuleList()\n",
    "        for out_channels, kernel_size in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(\n",
    "                # 两个嵌⼊的层连接,故in_channels=2 * embed_size\n",
    "                nn.Conv1d(in_channels=2 * embed_size, out_channels=out_channels, kernel_size=kernel_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape=(N, L);其实L表示序列长度\n",
    "        # 沿着向量维度将两个嵌⼊层连接起来\n",
    "        # embeddings.shape=(N, L, 2 * C);其中C表示输出词向量的维度大小\n",
    "        embeddings = torch.cat((self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "        # 根据⼀维卷积层的输⼊格式,重新排列张量,以便通道作为第2维\n",
    "        # embeddings.shape(N, 2 * C, L);\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # conv(embeddings).shape=(N, out_channels, L_out);其中out_channelsh表示输出通道数,L_out表示每个输出通道的宽度\n",
    "        # self.pool(conv(embeddings)).shape=(N, output_channels, 1)\n",
    "        # torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1).shape=(N, output_channels)\n",
    "        # encoding.shape=(N, output_channels1 + output_channels2 + output_channels3 + .......)\n",
    "        encoding = torch.cat([torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1) for conv in self.convs],\n",
    "                             dim=1)\n",
    "        # outputs.shape=(N, 14)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6bb4cc-3f73-4268-9f22-de654f7c9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes, nums_channels = [3, 4, 5], [128, 128, 128]  # 卷积核大小和输出通道\n",
    "\n",
    "net = TextCNN(pretrained_vector.shape[0], pretrained_vector.shape[1], kernel_sizes, nums_channels)\n",
    "net.embedding.weight.data.copy_(pretrained_vector)\n",
    "net.constant_embedding.weight.data.copy_(pretrained_vector)  # 使用预训练词向量矩阵\n",
    "net.constant_embedding.weight.requires_grad = False  # 冻结网络层,使之不参与训练\n",
    "net = net.to(device)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "criterion_cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b83d85b-c150-4324-ace8-50f2c24cd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (text, labels) in enumerate(dataloader):\n",
    "        # 数据设备切换\n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(text)\n",
    "        loss = criterion(out, labels)  # 每个step的损失值\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            predict = out.argmax(dim=1).cpu().numpy()\n",
    "            f1 = f1_score(labels.cpu().numpy(), predict, average='micro')  # 评估指标\n",
    "            print('| step {:5d} | loss {:8.3f} | f1 {:8.3f} |'.format(idx, loss.item(), f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891f629f-57c1-4eee-9319-45e21fb9e1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step    50 | loss    1.657 | f1    0.680 |\n",
      "| step   100 | loss    1.209 | f1    0.793 |\n",
      "| step   150 | loss    0.926 | f1    0.797 |\n",
      "| step   200 | loss    0.775 | f1    0.840 |\n",
      "| step   250 | loss    0.847 | f1    0.801 |\n",
      "| step   300 | loss    0.793 | f1    0.797 |\n",
      "| step   350 | loss    0.558 | f1    0.859 |\n",
      "| step   400 | loss    0.665 | f1    0.828 |\n",
      "| step   450 | loss    0.584 | f1    0.832 |\n",
      "| step   500 | loss    0.481 | f1    0.863 |\n",
      "| step   550 | loss    0.885 | f1    0.781 |\n",
      "| step   600 | loss    0.496 | f1    0.852 |\n",
      "| step   650 | loss    0.549 | f1    0.852 |\n",
      "| step   700 | loss    0.618 | f1    0.855 |\n",
      "| step   750 | loss    0.457 | f1    0.855 |\n",
      "-------------------------------------\n",
      "| end of epoch     1 | time: 452.32s |\n",
      "-------------------------------------\n",
      "| step    50 | loss    0.501 | f1    0.863 |\n",
      "| step   100 | loss    0.422 | f1    0.867 |\n",
      "| step   150 | loss    0.443 | f1    0.867 |\n",
      "| step   200 | loss    0.606 | f1    0.840 |\n",
      "| step   250 | loss    0.357 | f1    0.895 |\n",
      "| step   300 | loss    0.539 | f1    0.824 |\n",
      "| step   350 | loss    0.300 | f1    0.898 |\n",
      "| step   400 | loss    0.340 | f1    0.914 |\n",
      "| step   450 | loss    0.372 | f1    0.883 |\n",
      "| step   500 | loss    0.607 | f1    0.836 |\n",
      "| step   550 | loss    0.512 | f1    0.867 |\n",
      "| step   600 | loss    0.518 | f1    0.879 |\n",
      "| step   650 | loss    0.333 | f1    0.922 |\n",
      "| step   700 | loss    0.396 | f1    0.855 |\n",
      "| step   750 | loss    0.509 | f1    0.848 |\n",
      "-------------------------------------\n",
      "| end of epoch     2 | time: 444.49s |\n",
      "-------------------------------------\n",
      "| step    50 | loss    0.301 | f1    0.918 |\n",
      "| step   100 | loss    0.472 | f1    0.883 |\n",
      "| step   150 | loss    0.306 | f1    0.906 |\n",
      "| step   200 | loss    0.328 | f1    0.887 |\n",
      "| step   250 | loss    0.413 | f1    0.891 |\n",
      "| step   300 | loss    0.337 | f1    0.902 |\n",
      "| step   350 | loss    0.281 | f1    0.918 |\n",
      "| step   400 | loss    0.466 | f1    0.887 |\n",
      "| step   450 | loss    0.240 | f1    0.938 |\n",
      "| step   500 | loss    0.484 | f1    0.887 |\n",
      "| step   550 | loss    0.362 | f1    0.883 |\n",
      "| step   600 | loss    0.312 | f1    0.918 |\n",
      "| step   650 | loss    0.288 | f1    0.910 |\n",
      "| step   700 | loss    0.269 | f1    0.922 |\n",
      "| step   750 | loss    0.279 | f1    0.926 |\n",
      "-------------------------------------\n",
      "| end of epoch     3 | time: 454.11s |\n",
      "-------------------------------------\n",
      "| step    50 | loss    0.349 | f1    0.910 |\n",
      "| step   100 | loss    0.391 | f1    0.887 |\n",
      "| step   150 | loss    0.220 | f1    0.930 |\n",
      "| step   200 | loss    0.342 | f1    0.918 |\n",
      "| step   250 | loss    0.397 | f1    0.883 |\n",
      "| step   300 | loss    0.304 | f1    0.906 |\n",
      "| step   350 | loss    0.192 | f1    0.938 |\n",
      "| step   400 | loss    0.282 | f1    0.926 |\n",
      "| step   450 | loss    0.232 | f1    0.938 |\n",
      "| step   500 | loss    0.226 | f1    0.934 |\n",
      "| step   550 | loss    0.217 | f1    0.926 |\n",
      "| step   600 | loss    0.254 | f1    0.918 |\n",
      "| step   650 | loss    0.243 | f1    0.930 |\n",
      "| step   700 | loss    0.239 | f1    0.930 |\n",
      "| step   750 | loss    0.387 | f1    0.898 |\n",
      "-------------------------------------\n",
      "| end of epoch     4 | time: 444.27s |\n",
      "-------------------------------------\n",
      "| step    50 | loss    0.306 | f1    0.918 |\n",
      "| step   100 | loss    0.182 | f1    0.945 |\n",
      "| step   150 | loss    0.238 | f1    0.914 |\n",
      "| step   200 | loss    0.293 | f1    0.941 |\n",
      "| step   250 | loss    0.395 | f1    0.891 |\n",
      "| step   300 | loss    0.183 | f1    0.938 |\n",
      "| step   350 | loss    0.234 | f1    0.922 |\n",
      "| step   400 | loss    0.347 | f1    0.898 |\n",
      "| step   450 | loss    0.222 | f1    0.953 |\n",
      "| step   500 | loss    0.403 | f1    0.922 |\n",
      "| step   550 | loss    0.257 | f1    0.926 |\n",
      "| step   600 | loss    0.289 | f1    0.926 |\n",
      "| step   650 | loss    0.198 | f1    0.945 |\n",
      "| step   700 | loss    0.455 | f1    0.891 |\n",
      "| step   750 | loss    0.170 | f1    0.934 |\n",
      "-------------------------------------\n",
      "| end of epoch     5 | time: 444.65s |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(net, dataloader_tr, criterion_cross_entropy, optimizer, device)\n",
    "    print('-' * 37)\n",
    "    print('| end of epoch {:5d} | time: {:5.2f}s |'.format(epoch, time.time() - epoch_start_time))\n",
    "    print('-' * 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e4276ea-1d1d-4e37-a2e0-8690905a07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for text, in dataloader:\n",
    "            # 数据设备切换\n",
    "            text = text.to(device)\n",
    "            out = model(text)\n",
    "            predict_list.append(out.cpu())\n",
    "\n",
    "    predict_all = torch.cat(predict_list, dim=0)  # 合并所有批次的预测结果\n",
    "    return predict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "509d8418-c503-43af-8af7-9cae1117c8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6695, 23.9957,  1.4339,  ..., -6.5855,  1.1423, -6.3834],\n",
       "        [ 1.0942,  1.8273, 25.4002,  ..., -0.6347, 12.9418, -3.3128],\n",
       "        [ 4.6445,  2.7103,  0.3172,  ...,  3.1960, -3.6932, -6.8945],\n",
       "        ...,\n",
       "        [ 4.8764, 13.9193,  2.7801,  ..., -8.4020, -0.8942, -6.7221],\n",
       "        [ 4.3124, -0.0654,  8.0356,  ...,  3.4989, -4.7454, -3.8056],\n",
       "        [ 6.2089, 15.2082, -0.2523,  ..., -0.6511, -1.1270, -3.5069]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_te = Data.TensorDataset(torch.tensor(X_test_data.values.tolist()))\n",
    "dataloader_te = Data.DataLoader(dataset_te, 64)  # 测试数据集\n",
    "\n",
    "result_pro = predict(net, dataloader_te, device)\n",
    "result_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "236ae6a4-5d72-45e3-bde8-b6953c1627d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0          1\n",
       "1          2\n",
       "2          8\n",
       "3          5\n",
       "4          0\n",
       "...      ...\n",
       "49995      0\n",
       "49996     13\n",
       "49997      1\n",
       "49998      3\n",
       "49999      1\n",
       "\n",
       "[50000 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_result_label = np.argmax(result_pro.cpu().numpy(), axis=1)\n",
    "pro_result_label = pd.DataFrame(pro_result_label, columns=['label'])\n",
    "pro_result_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe57c41f-ea42-4c35-ae3f-eef182e6a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前3000 f1 score:0.9352\n",
    "# 前1500 + 后1500 f1 score:0.9343\n",
    "# 前6000 f1 score:0.9352\n",
    "pro_result_label.to_csv('../predict_result/textcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791f619-767d-420d-941b-cfc2759ffadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}