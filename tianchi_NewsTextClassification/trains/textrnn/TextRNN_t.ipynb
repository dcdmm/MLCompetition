{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1e13fd-5b4e-4087-9678-7426258ee272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10045e9e-38bd-44d1-8e27-7230bea05970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种\\\n",
    "\n",
    "\n",
    "seed = 2022\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d602bc5a-346e-4109-bc8a-96bca14a0910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce21a264-df7a-4e70-b942-3e64f370cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用10000条数据进行debug\n",
    "tr_index = np.arange(200000)\n",
    "np.random.shuffle(tr_index)\n",
    "\n",
    "train_df = pd.read_csv('../../datasets/train_set.csv', sep='\\t').iloc[tr_index[:10000], :]\n",
    "\n",
    "train_df.index = np.arange(10000)\n",
    "\n",
    "# 使用2500条数据进行debug\n",
    "test_df = pd.read_csv('../../datasets/test_a.csv', sep='\\t').iloc[:2500, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3d9ac4-2301-4b7d-88bf-39056447c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1149\n",
      "2416\n",
      "6336\n"
     ]
    }
   ],
   "source": [
    "# 加载word2vec字典\n",
    "load_vocal = joblib.load('../../data/vocab/vocab_word2vec.pkl')\n",
    "\n",
    "print(load_vocal.get_stoi().get('349', 0))\n",
    "print(load_vocal.get_stoi().get('3113', 0))\n",
    "print(load_vocal.get_stoi().get('4806', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b76d951-2aef-418b-8aa1-de54849baf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_truncate_pad(string,\n",
    "                       num_steps,  # 句子最大长度\n",
    "                       stoi,  # Dictionary mapping tokens to indices.\n",
    "                       padding_index):  # 填充字符'<pad>'在词典中的索引\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    # 获取字在Vocab对象中的位置\n",
    "    line = [stoi.get(word, 0) for word in string.split()]\n",
    "    if len(line) > num_steps:\n",
    "        # 直接返回列表速度较快\n",
    "        return line[:4000]\n",
    "        # return line[:num_steps]  # 句子截断\n",
    "    return line + [padding_index] * (num_steps - len(line))  # 句子填充\n",
    "\n",
    "\n",
    "X_train_data = train_df['text'].apply(split_truncate_pad, num_steps=4000,\n",
    "                                      stoi=load_vocal.get_stoi(), padding_index=1)  # 这里设置句子最大长度为4000\n",
    "X_test_data = test_df['text'].apply(split_truncate_pad, num_steps=4000, stoi=load_vocal.get_stoi(),\n",
    "                                    padding_index=1)\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "917eaa29-e19b-47e8-94bb-59e8a54b940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8654,  1.0556, -1.1478,  ...,  2.9752, -1.3487, -1.1243],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预训练词向量文件\n",
    "vector = torchtext.vocab.Vectors(name=\"cnew_300.txt\",\n",
    "                                 cache='../word2vec')\n",
    "\n",
    "pretrained_vector = vector.get_vecs_by_tokens(load_vocal.get_itos())\n",
    "pretrained_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c42d5fc-1b8a-412a-a18a-dccc012975f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN_MeanMaxPool(nn.Module):\n",
    "    \"\"\"\n",
    "    TextRNN + [MeanPool, MaxPool]模型的pytorch实现(具体任务对应修改)\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    num_class : int\n",
    "       类别数\n",
    "    vocab_size : int\n",
    "        单词表的单词数目\n",
    "    embedding_size : int\n",
    "        输出词向量的维度大小\n",
    "    hidden_size : int\n",
    "        隐含变量的维度大小(权重矩阵W_{ih}、W_{hh}中h的大小)\n",
    "    num_layers : int\n",
    "        循环神经网络层数\n",
    "    bidirectional : bool\n",
    "        是否为设置为双向循环神经网络\n",
    "    dropout_ratio : float\n",
    "        元素归零的概率\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, vocab_size, embedding_size, hidden_size, num_layers, bidirectional, dropout_ratio):\n",
    "        super(TextRNN_MeanMaxPool, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.GRU(input_size=embedding_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bidirectional=self.bidirectional,\n",
    "                          dropout=dropout_ratio,\n",
    "                          batch_first=True)  # batch_size为第一个维度\n",
    "\n",
    "        if self.bidirectional:\n",
    "            mul = 2\n",
    "        else:\n",
    "            mul = 1\n",
    "        self.linear1 = nn.Linear(hidden_size * mul * 2, 1024)\n",
    "        self.linear2 = nn.Linear(1024, num_class)\n",
    "        self.norm = nn.LayerNorm(normalized_shape=1024)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text.shape=[batch_size, sent len]\n",
    "\n",
    "        # embedded.shape=[batch_size, sen len, embedding_size]\n",
    "        embedded = self.dropout(self.embed(text))\n",
    "        # out.shape=[batch_size, sen len, hidden_size * num directions]  # 即h_{it}\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        feat_mean = torch.mean(out, dim=1)\n",
    "        feat_max = torch.max(out, dim=1).values\n",
    "\n",
    "        result = torch.cat((feat_mean, feat_max), 1)\n",
    "        result = self.linear1(result)\n",
    "        result = self.norm(result)\n",
    "        result = self.relu(result)\n",
    "        result = self.dropout(result)\n",
    "        # result.shape=[batch_size, num_class]\n",
    "        result = self.linear2(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c378b5d7-6a70-4d9e-b11c-7ad4d6c813bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # 学习率线性衰减(最小为0)\n",
    "        # num_training_steps后学习率恒为0\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bf9f672-d844-4854-a574-225082c3a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    \"\"\"Fast Gradient Sign Method\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self,\n",
    "               emb_name,  # 添加扰动的embedding层名称\n",
    "               epsilon=1.0):  # 扰动项中的\\epsilon\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and name == emb_name:\n",
    "                self.backup[name] = param.detach().clone()\n",
    "                norm = torch.linalg.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm  # \\epsilon * (g / ||g||_2)\n",
    "                    param.data.add_(r_at)  # embedding层参数增加扰动\\Delta x\n",
    "\n",
    "    def restore(self, emb_name):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and name == emb_name:\n",
    "                param.data = self.backup[name]  # 恢复embedding层原有参数值\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1bc48f-dabf-4391-9ce5-d77732162fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, device):\n",
    "    # FGM对抗训练step 1\n",
    "    fgm = FGM(model)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idx, (text, labels) in enumerate(dataloader):\n",
    "        # 数据设备切换\n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(text)\n",
    "        loss = criterion(out, labels)  # 每个step的损失值\n",
    "        loss.backward()\n",
    "\n",
    "        # FGM对抗训练step 2\n",
    "        fgm.attack(emb_name='embed.weight', epsilon=1.)\n",
    "        out_adv = model(text)\n",
    "        loss_adv = criterion(out_adv, labels)\n",
    "        loss_adv.backward()  # 对抗训练的梯度\n",
    "        fgm.restore(emb_name='embed.weight')  # 恢复embedding层原有参数值\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # mmmmmm\n",
    "        if idx % 50 == 0 and idx > 0:  # 每50个step评估一次f1 score\n",
    "            predict = out.argmax(dim=1).cpu().numpy()\n",
    "            f1 = f1_score(labels.cpu().numpy(), predict, average='micro')\n",
    "            print('| step {:5d} | loss {:8.3f} | f1 {:8.3f} |'.format(idx, loss.item(), f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4108131c-4224-4017-a28c-f2ff4bfb0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    predict_list = []\n",
    "    y_true_list = []\n",
    "    with torch.no_grad():\n",
    "        for text, labels in dataloader:\n",
    "            # 数据设备切换\n",
    "            text = text.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(text)\n",
    "            predict_list.append(out.cpu())\n",
    "            y_true_list.extend(labels.tolist())\n",
    "\n",
    "    predict_all = torch.cat(predict_list, dim=0)  # 合并所有批次的预测结果\n",
    "    y_true_all = torch.tensor(y_true_list)  # 真实标签\n",
    "    f1 = f1_score(y_true_all.numpy(), predict_all.argmax(dim=1).numpy(), average='micro')  # 验证数据集f1 score\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f4c15c-1bd2-4f10-86d5-5003cc60c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************0****************************************\n",
      "| step    50 | loss    2.143 | f1    0.375 |\n",
      "| step   100 | loss    1.403 | f1    0.516 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     1 | time: 142.01s | valid f1  0.72250 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.520 | f1    0.844 |\n",
      "| step   100 | loss    0.554 | f1    0.875 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     2 | time: 148.47s | valid f1  0.88750 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.297 | f1    0.891 |\n",
      "| step   100 | loss    0.231 | f1    0.891 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     3 | time: 149.99s | valid f1  0.91900 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.128 | f1    0.984 |\n",
      "| step   100 | loss    0.078 | f1    0.984 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     4 | time: 153.62s | valid f1  0.93050 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.076 | f1    0.984 |\n",
      "| step   100 | loss    0.035 | f1    1.000 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     5 | time: 140.41s | valid f1  0.94200 |\n",
      "----------------------------------------------------------\n",
      "****************************************1****************************************\n",
      "| step    50 | loss    2.020 | f1    0.281 |\n",
      "| step   100 | loss    1.594 | f1    0.484 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     1 | time: 149.80s | valid f1  0.75450 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.266 | f1    0.953 |\n",
      "| step   100 | loss    0.197 | f1    0.953 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     2 | time: 144.40s | valid f1  0.89250 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.402 | f1    0.891 |\n",
      "| step   100 | loss    0.287 | f1    0.938 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     3 | time: 142.33s | valid f1  0.91850 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.128 | f1    0.953 |\n",
      "| step   100 | loss    0.132 | f1    0.969 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     4 | time: 147.49s | valid f1  0.92450 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.028 | f1    1.000 |\n",
      "| step   100 | loss    0.057 | f1    0.969 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     5 | time: 148.82s | valid f1  0.93100 |\n",
      "----------------------------------------------------------\n",
      "****************************************2****************************************\n",
      "| step    50 | loss    2.146 | f1    0.297 |\n",
      "| step   100 | loss    1.574 | f1    0.453 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     1 | time: 148.77s | valid f1  0.74700 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.524 | f1    0.844 |\n",
      "| step   100 | loss    0.466 | f1    0.875 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     2 | time: 145.53s | valid f1  0.89350 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.178 | f1    0.938 |\n",
      "| step   100 | loss    0.214 | f1    0.953 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     3 | time: 151.62s | valid f1  0.90800 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.077 | f1    0.969 |\n",
      "| step   100 | loss    0.086 | f1    0.969 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     4 | time: 151.48s | valid f1  0.92950 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.084 | f1    0.969 |\n",
      "| step   100 | loss    0.044 | f1    0.969 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     5 | time: 140.07s | valid f1  0.92750 |\n",
      "----------------------------------------------------------\n",
      "****************************************3****************************************\n",
      "| step    50 | loss    2.278 | f1    0.375 |\n",
      "| step   100 | loss    1.274 | f1    0.641 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     1 | time: 151.24s | valid f1  0.74750 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.724 | f1    0.844 |\n",
      "| step   100 | loss    0.455 | f1    0.891 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     2 | time: 148.10s | valid f1  0.91050 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.318 | f1    0.891 |\n",
      "| step   100 | loss    0.293 | f1    0.906 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     3 | time: 150.98s | valid f1  0.91450 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.110 | f1    0.984 |\n",
      "| step   100 | loss    0.296 | f1    0.891 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     4 | time: 146.21s | valid f1  0.92650 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.021 | f1    1.000 |\n",
      "| step   100 | loss    0.017 | f1    1.000 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     5 | time: 152.72s | valid f1  0.93650 |\n",
      "----------------------------------------------------------\n",
      "****************************************4****************************************\n",
      "| step    50 | loss    2.041 | f1    0.422 |\n",
      "| step   100 | loss    1.449 | f1    0.562 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     1 | time: 150.77s | valid f1  0.73600 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.337 | f1    0.938 |\n",
      "| step   100 | loss    0.217 | f1    0.938 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     2 | time: 142.94s | valid f1  0.88550 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.303 | f1    0.922 |\n",
      "| step   100 | loss    0.275 | f1    0.875 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     3 | time: 149.75s | valid f1  0.90900 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.080 | f1    0.953 |\n",
      "| step   100 | loss    0.068 | f1    1.000 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     4 | time: 145.69s | valid f1  0.92100 |\n",
      "----------------------------------------------------------\n",
      "| step    50 | loss    0.050 | f1    0.984 |\n",
      "| step   100 | loss    0.022 | f1    1.000 |\n",
      "----------------------------------------------------------\n",
      "| end of epoch     5 | time: 145.24s | valid f1  0.91950 |\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  # 5折分层交叉验证\n",
    "best_valid_f1_lst = []  # 每折交叉验证最佳模型验证数据集的f1 score\n",
    "best_model_state_dict_lst = []  # 每折交叉验证最佳模型的状态字典\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(skfold.split(X_train_data, y_train)):\n",
    "    print('*' * 40 + str(fold) + '*' * 40)\n",
    "\n",
    "    tr_d, va_d = X_train_data.iloc[trn_ind], X_train_data[val_ind]\n",
    "    tr_y, va_y = y_train[trn_ind], y_train[val_ind]\n",
    "\n",
    "    dataset_tr = Data.TensorDataset(torch.tensor(tr_d.values.tolist()), torch.tensor(tr_y))\n",
    "    dataloader_tr = Data.DataLoader(dataset_tr, 64, shuffle=True)\n",
    "    dataset_va = Data.TensorDataset(torch.tensor(va_d.values.tolist()), torch.tensor(va_y))\n",
    "    dataloader_va = Data.DataLoader(dataset_va, 64)\n",
    "\n",
    "    # *************************************************************************************************************\n",
    "    vocal_size, embedding_size = pretrained_vector.shape\n",
    "    hidden_size = 256\n",
    "    dropout = 0.2\n",
    "    bidirectional = True\n",
    "    num_class = 14\n",
    "    num_layers = 2\n",
    "\n",
    "    net = TextRNN_MeanMaxPool(num_class=num_class,\n",
    "                              vocab_size=vocal_size,\n",
    "                              embedding_size=embedding_size,\n",
    "                              hidden_size=hidden_size,\n",
    "                              num_layers=num_layers,\n",
    "                              dropout_ratio=dropout,\n",
    "                              bidirectional=True)\n",
    "    net.embed.weight.data.copy_(pretrained_vector)  # 使用预训练词向量矩阵\n",
    "    net = net.to(device)\n",
    "\n",
    "    params_1x = [param for name, param in net.named_parameters() if name not in [\"embed.weight\"]]\n",
    "    optimizer = torch.optim.Adam([{'params': params_1x, 'lr': 0.001},\n",
    "                                  {'params': net.embed.parameters(), 'lr': 2e-5}])  # 预训练词向量使用更低的学习率\n",
    "    scheduler_lr = get_linear_schedule_with_warmup(optimizer, len(dataloader_tr) * 2, len(dataloader_tr) * 5)\n",
    "\n",
    "    criterion_cross_entropy = nn.CrossEntropyLoss()\n",
    "    # *************************************************************************************************************\n",
    "\n",
    "    best_valid_f1 = 0.0  # 最佳模型验证数据集的f1 score\n",
    "    best_model_state_dict = [None]  # 最佳模型的状态字典\n",
    "    EPOCHS = 5\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(net, dataloader_tr, criterion_cross_entropy, optimizer, scheduler_lr, device)\n",
    "        va_f1 = evaluate(net, dataloader_va, device)\n",
    "        if va_f1 > best_valid_f1:\n",
    "            best_valid_f1 = va_f1\n",
    "            best_model_state_dict.pop()\n",
    "            best_model_state_dict.append(copy.deepcopy(net.state_dict()))  # 必须进行深拷贝\n",
    "        print('-' * 58)\n",
    "        print('| end of epoch {:5d} | time: {:5.2f}s | valid f1 {:8.5f} |'.format(epoch, time.time() - epoch_start_time,\n",
    "                                                                                  va_f1))\n",
    "        print('-' * 58)\n",
    "\n",
    "    best_valid_f1_lst.append(best_valid_f1)\n",
    "    best_model_state_dict_lst.extend(best_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dfaff71-7858-4e9f-9aca-53fb3033070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for text, in dataloader:\n",
    "            # 数据设备切换\n",
    "            text = text.to(device)\n",
    "            out = model(text)\n",
    "            predict_list.append(out.cpu())\n",
    "\n",
    "    predict_all = torch.cat(predict_list, dim=0)  # 合并所有批次的预测结果\n",
    "    return predict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b47a7a9a-a95d-4f6f-9ece-8d9e90eccca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3412,  9.9544, -3.7086,  ..., -3.9090, -0.3331, -6.6511],\n",
       "        [-3.1288, -2.4968,  9.8460,  ..., -1.6676,  2.5009, -1.2129],\n",
       "        [ 0.5633, -1.9388, -4.5436,  ...,  1.1361, -4.4918, -1.7583],\n",
       "        ...,\n",
       "        [ 0.9948, 10.0543, -4.1693,  ..., -2.5158, -0.4458, -5.6663],\n",
       "        [-0.1890, -3.2594, -0.3849,  ..., -2.4763, -5.6685, -2.2568],\n",
       "        [-2.6831, -3.8079,  9.4572,  ..., -1.5826,  0.1616, -1.2957]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_te = Data.TensorDataset(torch.tensor(X_test_data.values.tolist()))\n",
    "dataloader_te = Data.DataLoader(dataset_te, 64)  # 测试数据集\n",
    "\n",
    "result_pro = torch.zeros((X_test_data.values.shape[0], 14))\n",
    "for model_state_dict_i in best_model_state_dict_lst:\n",
    "    # *************************************************************************************************************\n",
    "    vocal_size, embedding_size = pretrained_vector.shape\n",
    "    hidden_size = 256\n",
    "    dropout = 0.2\n",
    "    bidirectional = True\n",
    "    num_class = 14\n",
    "    num_layers = 2\n",
    "\n",
    "    net_i = TextRNN_MeanMaxPool(num_class=num_class,\n",
    "                                vocab_size=vocal_size,\n",
    "                                embedding_size=embedding_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                dropout_ratio=dropout,\n",
    "                                bidirectional=True)\n",
    "    net_i.embed.weight.data.copy_(pretrained_vector)  # 使用预训练词向量矩阵\n",
    "    net_i = net_i.to(device)\n",
    "\n",
    "    net_i.load_state_dict(model_state_dict_i)  # 模型加载\n",
    "    # *************************************************************************************************************\n",
    "    result_pro += predict(net_i, dataloader_te, device) / skfold.n_splits\n",
    "result_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e74b34a1-6213-45ea-9961-be9989496c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "0         1\n",
       "1         2\n",
       "2         8\n",
       "3         5\n",
       "4         0\n",
       "...     ...\n",
       "2495      3\n",
       "2496      2\n",
       "2497      1\n",
       "2498      3\n",
       "2499      2\n",
       "\n",
       "[2500 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_result_label = np.argmax(result_pro.cpu().numpy(), axis=1)\n",
    "pre_result_label = pd.DataFrame(pre_result_label, columns=['label'])\n",
    "pre_result_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe91892-55b6-44d6-ad43-2a1bc0590d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_result_label.to_csv('../../predict_result/textrnn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}