{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa59051-8451-488e-a499-3aa80952b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89ddfad-577a-455b-81a6-0f74127ec9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))  # 分词器\n",
    "# This pre-tokenizer simply splits using the following regex: \\w+|[^\\w\\s]+\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n",
    "tokenizer.train(['../../data/csv_to_trainTxt/trainTxt_pretrain_model.txt'], trainer)\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(single=\"<s> $A </s>\",\n",
    "                                              pair=\"<s> $A </s> </s> $B:1 </s>:1\",\n",
    "                                              special_tokens=[(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "                                                              (\"</s>\", tokenizer.token_to_id(\"</s>\"))])\n",
    "\n",
    "print(tokenizer.token_to_id('<pad>'))\n",
    "\n",
    "# Enable the padding\n",
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id('<pad>'), pad_token=\"<pad>\", pad_type_id=0)\n",
    "\n",
    "# Enable truncation\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d1a7d6-6442-4dcc-ae78-cb93cbf15a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '5399', '3117', '1070', '4321', '4568', '2621', '5466', '3772', '4516', '2990', '3618', '2456', '</s>']\n",
      "[1, 1575, 582, 156, 430, 537, 299, 918, 125, 54, 583, 109, 355, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output_temp = tokenizer.encode(sequence=\"5399 3117 1070 4321 4568 2621 5466 3772 4516 2990 3618 2456\",\n",
    "                               is_pretokenized=False)\n",
    "\n",
    "print(output_temp.tokens)\n",
    "print(output_temp.ids)\n",
    "print(output_temp.type_ids)\n",
    "print(output_temp.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c130b04c-b9c1-4be4-96da-56cc51962cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}