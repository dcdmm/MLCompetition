{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2b2c7b-2666-4109-bd1c-949f438f4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, RobertaForMaskedLM, DataCollatorForLanguageModeling, AutoConfig\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6134279f-88d6-4e52-8ea4-7b64c7578a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=6982, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的分词器\n",
    "tokenizer_fast = PreTrainedTokenizerFast(tokenizer_file='tokenizer.json')\n",
    "\n",
    "# 模仿HuggingFace transformers/models/bert/tokenizaiton_bert.py\n",
    "tokenizer_fast.add_special_tokens(special_tokens_dict={'bos_token': \"<s>\",\n",
    "                                                       'eos_token': '</s>',\n",
    "                                                       'unk_token': '<unk>',\n",
    "                                                       'pad_token': '<pad>',\n",
    "                                                       'cls_token': '<s>',\n",
    "                                                       'mask_token': '<mask>',\n",
    "                                                       'sep_token': '</s>'})\n",
    "tokenizer_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0776f63d-2e2d-41f3-bbb0-ebaf24e78356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c427600ca8f54ad1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-c427600ca8f54ad1/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f70f185623441082325d61441effbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367ef9fb767f437ebd92a78f0d8dd0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-c427600ca8f54ad1/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 662554\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_text('../../data/csv_to_trainTxt/trainTxt_pretrain_model.txt')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efa584d-a8b8-4988-bd45-bea0100b8914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359d04b595b648fc89f538ea7a4d1606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/663 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27833a28b334006a3b79fdae987fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/570 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'token_type_ids'],\n",
       "    num_rows: 569598\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_func(data):\n",
    "    text = data['text']\n",
    "    return len(text) > 0 and not text.isspace()  # 过滤空白行\n",
    "\n",
    "\n",
    "def map_func(data):\n",
    "    batch_encoding = tokenizer_fast(data['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    return {'input_ids': batch_encoding['input_ids'],\n",
    "            'attention_mask': batch_encoding['attention_mask'],\n",
    "            'token_type_ids': batch_encoding['token_type_ids']}\n",
    "\n",
    "\n",
    "dataset_filter = dataset.filter(filter_func)\n",
    "dataset_map = dataset_filter.map(map_func, batched=True, batch_size=1000)  # 每次处理1000条数据\n",
    "dataset_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a6dcd8-8a7f-4029-9594-ddea743a83e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='', vocab_size=6982, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}), mlm=True, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator used for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer_fast, mlm=True, mlm_probability=0.15)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4839895-cb1a-4d8c-920c-fe006150bf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e95def56a1476a954c01609fa4a33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 6982\n",
      "}\n",
      "\n",
      "No of parameters:  91412806\n"
     ]
    }
   ],
   "source": [
    "config_roberta_base = AutoConfig.from_pretrained('roberta-base')  # 加载预训练模型roberta config参数\n",
    "config_roberta_base.update({\n",
    "    'vocab_size': tokenizer_fast.vocab_size\n",
    "})\n",
    "print(config_roberta_base)\n",
    "\n",
    "model = RobertaForMaskedLM(config_roberta_base)  # 从0开始预训练roberta模型\n",
    "# model = RobertaForMaskedLM.from_pretrained('output_idr/checkpoint-??????')  # 从checkpoint开始训练(之前模型训练中断)\n",
    "print('No of parameters: ', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997ad498-4806-4bb2-a0b8-287fa93521ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: text. If text are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/root/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 569598\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:16, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output_dir/checkpoint-50\n",
      "Configuration saved in output_dir/checkpoint-50/config.json\n",
      "Model weights saved in output_dir/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in output_dir/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=7.819699096679687, metrics={'train_runtime': 17.9658, 'train_samples_per_second': 44.529, 'train_steps_per_second': 2.783, 'total_flos': 210505998336000.0, 'train_loss': 7.819699096679687, 'epoch': 0.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output_dir',\n",
    "    overwrite_output_dir=True,\n",
    "    # num_train_epochs=30.0,\n",
    "    max_steps=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_strategy='epoch',\n",
    "    disable_tqdm=False,  # 是否使用tqdm显示进度\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_map,\n",
    "    tokenizer=tokenizer_fast\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b8b77-2547-4c38-bc0d-96c3ed80e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"save_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}