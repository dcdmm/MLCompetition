{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560f0c3c-f451-4e49-b2f3-614bfc52436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (RobertaForMaskedLM, RobertaTokenizer, DataCollatorForLanguageModeling, Trainer,\n",
    "                          TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79e2c84-1572-41c6-8b17-5bb1ee2fe663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "model_name = 'roberta-large'\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)  # 已经训练好的预训练模型\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53849ad-24d3-455b-96c4-4e08ce3632af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       When the young people returned to the ballroom...\n",
       "1       All through dinner time, Mrs. Fayre was somewh...\n",
       "2       As Roger had predicted, the snow departed as q...\n",
       "3       And outside before the palace a great garden w...\n",
       "4       Once upon a time there were Three Bears who li...\n",
       "                              ...                        \n",
       "2836    It was a bright and cheerful scene that greete...\n",
       "2837    Cell division is the process by which a parent...\n",
       "2838    Debugging is the process of finding and resolv...\n",
       "2839    To explain transitivity, let us look first at ...\n",
       "2840    Milka and John are playing in the garden. Her ...\n",
       "Name: excerpt, Length: 2841, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../../datasets/train.csv')['excerpt']\n",
    "test_data = pd.read_csv('../../datasets/test.csv')['excerpt']\n",
    "all_data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18431832-89f9-4a1b-a034-ef7334cc256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2841\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(pd.DataFrame(all_data.values, columns=['text']))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e30bd6-e5d7-4871-8037-994b8f02f816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b76b2e9d3e8431d9f5b61feb0977476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c282c9cab7423c90aedf8c329f9151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_func(data):\n",
    "    text = data['text']\n",
    "    return len(text) > 0 and not text.isspace()  # 过滤空白行\n",
    "\n",
    "\n",
    "def map_func(data):\n",
    "    batch_encoding = tokenizer(data['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    return {'input_ids': batch_encoding['input_ids'],\n",
    "            'attention_mask': batch_encoding['attention_mask']}\n",
    "\n",
    "\n",
    "dataset_filter = dataset.filter(filter_func)\n",
    "dataset_map = dataset_filter.map(map_func, batched=True, batch_size=1000)  # 每次处理1000条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18b51c6-642e-4465-ad20-d00dcf2e86d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizer(name_or_path='roberta-large', vocab_size=50265, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)}), mlm=True, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator used for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a14e6a-b846-4684-9b64-4380cbf73e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: text. If text are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/root/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2841\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1780' max='1780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1780/1780 16:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>1.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>1.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1068</td>\n",
       "      <td>1.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1424</td>\n",
       "      <td>1.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.099700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output_dir/checkpoint-356\n",
      "Configuration saved in output_dir/checkpoint-356/config.json\n",
      "Model weights saved in output_dir/checkpoint-356/pytorch_model.bin\n",
      "tokenizer config file saved in output_dir/checkpoint-356/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-356/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-712\n",
      "Configuration saved in output_dir/checkpoint-712/config.json\n",
      "Model weights saved in output_dir/checkpoint-712/pytorch_model.bin\n",
      "tokenizer config file saved in output_dir/checkpoint-712/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-712/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-1068\n",
      "Configuration saved in output_dir/checkpoint-1068/config.json\n",
      "Model weights saved in output_dir/checkpoint-1068/pytorch_model.bin\n",
      "tokenizer config file saved in output_dir/checkpoint-1068/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-1068/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-1424\n",
      "Configuration saved in output_dir/checkpoint-1424/config.json\n",
      "Model weights saved in output_dir/checkpoint-1424/pytorch_model.bin\n",
      "tokenizer config file saved in output_dir/checkpoint-1424/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-1424/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-1780\n",
      "Configuration saved in output_dir/checkpoint-1780/config.json\n",
      "Model weights saved in output_dir/checkpoint-1780/pytorch_model.bin\n",
      "tokenizer config file saved in output_dir/checkpoint-1780/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-1780/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1780, training_loss=1.260038997350114, metrics={'train_runtime': 987.8622, 'train_samples_per_second': 14.38, 'train_steps_per_second': 1.802, 'total_flos': 1.324027838080512e+16, 'train_loss': 1.260038997350114, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output_dir',\n",
    "    overwrite_output_dir=True,\n",
    "    seed=42,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    disable_tqdm=False  # 是否使用tqdm显示进度\n",
    ")\n",
    "\n",
    "# 继续训练预训练模型\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_map,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebe8a3d-d4d5-464b-a99b-7d620c4defb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in save_model/config.json\n",
      "Model weights saved in save_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('save_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184e347-2b97-4d83-9275-e69dccadb538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}