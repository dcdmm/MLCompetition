{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff3fdd7-b7f5-4d28-9ef2-a82cde5940a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import copy\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a56537-1c56-4550-ab87-8164bdd5fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"PyTorch随机数种子设置大全\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # CPU上设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # 当前GPU上设置随机种子\n",
    "        # A bool that, if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # torch.cuda.manual_seed_all(seed) # 所有GPU上设置随机种子\n",
    "\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae0ff35-0723-426d-afbf-c2a8427e8f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c98767-dae4-4227-a7fb-534fcbf29bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a bright and cheerful scene that greete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cell_division</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Cell division is the process by which a parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Debugging</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Debugging is the process of finding and resolv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    url_legal       license  \\\n",
       "0  c0f722661                                          NaN           NaN   \n",
       "1  f0953f0a5                                          NaN           NaN   \n",
       "2  0df072751                                          NaN           NaN   \n",
       "3  04caf4e0c  https://en.wikipedia.org/wiki/Cell_division  CC BY-SA 3.0   \n",
       "4  0e63f8bea      https://en.wikipedia.org/wiki/Debugging  CC BY-SA 3.0   \n",
       "\n",
       "                                             excerpt  \n",
       "0  My hope lay in Jack's promise that he would ke...  \n",
       "1  Dotty continued to go to Mrs. Gray's every nig...  \n",
       "2  It was a bright and cheerful scene that greete...  \n",
       "3  Cell division is the process by which a parent...  \n",
       "4  Debugging is the process of finding and resolv...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../../datasets/test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa3b6d5-0dac-42db-86c7-670da300a74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My hope lay in Jack's promise that he would keep a bright light burning in the upper story to guide me on my course. On a clear night this light was visible from the village, but somehow or other I failed to take into account the state of the weather. The air was full of eddying flakes, which would render the headlight of a locomotive invisible a hundred yards distant. Strange that this important fact never occurred to me until I was fully a fourth of a mile from the village. Then, after looking in vain for the beacon light, the danger of my situation struck me, and I halted.\n",
      "\"I am certain to go wrong,\" I said to myself.\n",
      "\"It is out of my power to follow a direct course without something to serve as a compass. I will go back to the village and wait till morning.\"\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Data.Dataset):\n",
    "    \"\"\"定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, df, have_target=True):\n",
    "        self.dataset = df\n",
    "        self.have_target = have_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"定义索引方式\"\"\"\n",
    "        text = self.dataset.iloc[i]['excerpt']\n",
    "        if self.have_target:\n",
    "            target = self.dataset.iloc[i]['target']\n",
    "            return text, target\n",
    "        else:\n",
    "            return text,\n",
    "\n",
    "\n",
    "data_test = MyDataset(df_test, have_target=False)\n",
    "\n",
    "for text, in data_test:\n",
    "    # 调用__getitem__方法\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0b778e-bf46-445a-9ba2-85dfad48343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcf4f658e054123b09ee51e08f0b6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334092288\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/electra-large-discriminator'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.model_input_names)\n",
    "\n",
    "pretrained = AutoModel.from_pretrained(model_name)\n",
    "print(pretrained.num_parameters())  # 参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f034d3a2-ef46-491c-907f-1f43ff3f8c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2026,  3246,  ...,     0,     0,     0],\n",
      "        [  101, 11089,  3723,  ...,     0,     0,     0],\n",
      "        [  101,  2009,  2001,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2139,  8569,  ...,     0,     0,     0],\n",
      "        [  101,  2000,  4863,  ...,     0,     0,     0],\n",
      "        [  101,  6501,  2050,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "def get_collate_fn(tokenizer, max_len=256):\n",
    "    \"\"\"返回collate_fun函数(通过闭包函数引入形参)\"\"\"\n",
    "\n",
    "    def collate_fn(data):\n",
    "        sents = [i[0] for i in data]\n",
    "\n",
    "        # 批量编码句子\n",
    "        text_t = tokenizer(text=sents,\n",
    "                           truncation=True,\n",
    "                           padding='max_length',\n",
    "                           max_length=max_len,\n",
    "                           return_token_type_ids=True,\n",
    "                           return_attention_mask=True,\n",
    "                           return_tensors='pt')\n",
    "\n",
    "        input_ids = text_t['input_ids']\n",
    "        attention_mask = text_t['attention_mask']\n",
    "        token_type_ids = text_t['token_type_ids']\n",
    "        if len(data[0]) == 1:\n",
    "            return input_ids, attention_mask, token_type_ids\n",
    "        else:\n",
    "            target = torch.tensor([i[1] for i in data], dtype=torch.float32)\n",
    "            return input_ids, attention_mask, token_type_ids, target\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=data_test, batch_size=8, collate_fn=get_collate_fn(tokenizer),\n",
    "                                              shuffle=False)\n",
    "\n",
    "for input_ids, attention_mask, token_type_ids in dataloader_test:\n",
    "    print(input_ids)\n",
    "    print(attention_mask)\n",
    "    print(token_type_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9341ead-9eb3-4b9f-8206-1fed869378b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.pretrained = pretrained_model\n",
    "        self.norm = nn.LayerNorm(1024)  # roberta-large隐藏层大小为1024\n",
    "        self.linear = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        emb = self.pretrained(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[\n",
    "            \"last_hidden_state\"]\n",
    "        emb = torch.mean(emb, dim=1)\n",
    "        output = self.norm(emb)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "731ec72d-c893-46cc-bc7e-cb10431d9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model,\n",
    "                   encoder_layer_init_lr=2e-5,  # bert模型最后一个encoder结构的学习率\n",
    "                   multiplier=0.95,  # 衰退因子\n",
    "                   custom_lr=1e-4):  # 自定义的网络层学习率\n",
    "    parameters = []\n",
    "    lr = encoder_layer_init_lr\n",
    "\n",
    "    # 自定义网络层:下游任务自定义的网络层(具体任务对应修改)\n",
    "    custom_params = {\n",
    "        'params': [param for name, param in model.named_parameters() if 'linear' in name or 'norm' in name],\n",
    "        'lr': custom_lr\n",
    "    }\n",
    "    parameters.append(custom_params)\n",
    "\n",
    "    # encoder层:\n",
    "    # bert-larger共有24个encoder结构(分别为encoder.layer.0, encoder.layer.1, ......, encoder.layer.23)\n",
    "    # bert-base共有12个encoder结构(分别为encoder.layer.0, encoder.layer.1, ......, encoder.layer.11)\n",
    "    for layer in range(23, -1, -1):\n",
    "        layer_params = {\n",
    "            'params': [param for name, param in model.named_parameters() if f'encoder.layer.{layer}.' in name],\n",
    "            'lr': lr\n",
    "        }\n",
    "        parameters.append(layer_params)\n",
    "        lr *= multiplier  # 上个encoder结构的学习率 = 该encoder结构的学习率 * 衰退因子\n",
    "\n",
    "    # embedding层:bert模型embedding层(最底层)\n",
    "    embeddings_params = {\n",
    "        'params': [param for name, param in model.named_parameters() if 'pretrained.embeddings' in name],  # 关键字in表示是否包含\n",
    "        'lr': 1e-7\n",
    "    }\n",
    "    parameters.append(embeddings_params)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31cb7b3-7809-4099-823b-0d4005c4b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
    "    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n",
    "    initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "        num_cycles (`float`, *optional*, defaults to 0.5):\n",
    "            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n",
    "            following a half-cosine).\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            # 学习率预热(线性增加)\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))  # 根据cos函数变化\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2e8938-1c10-4901-8838-55d269f339f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss_7 = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, target in dataloader:\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            loss = torch.sqrt(F.mse_loss(out.cpu().reshape(-1), target, reduction='mean'))\n",
    "            loss_7.append(loss.item())\n",
    "\n",
    "    return torch.mean(torch.tensor(loss_7)).item()  # 平均rmse\n",
    "\n",
    "\n",
    "# 每训练random.randint(5, 10)个step进行一次模型验证\n",
    "def train_and_evaluate(model, dataloader_train, dataloader_val, criterion, optimizer, scheduler_lr, epochs, device):\n",
    "    model.train()\n",
    "\n",
    "    best_valid_rmse = 1e7  # 最佳模型验证数据集的rmse\n",
    "    best_model_state_dict = [None]  # 最佳模型的状态字典\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        val_idx = random.randint(5, 10)\n",
    "        for idx, (input_ids, attention_mask, token_type_ids, target) in enumerate(dataloader_train):\n",
    "            # 数据设备切换\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            out = out.reshape(-1)\n",
    "\n",
    "            loss = criterion(out, target)  # 每个step的损失值\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler_lr.step()\n",
    "\n",
    "            if idx == val_idx:\n",
    "                val_idx += random.randint(8, 20)\n",
    "                val_rmse = evaluate(model, dataloader_val, device)\n",
    "                if val_rmse < best_valid_rmse:\n",
    "                    best_valid_rmse = val_rmse\n",
    "                    best_model_state_dict.pop()\n",
    "                    best_model_state_dict.append(copy.deepcopy(model.state_dict()))  # 状态字典必须进行深拷贝\n",
    "                    print('| end of epoch {:5d} | step: {:5d} | valild rmse {:8.5f} |'.format(epoch, idx, val_rmse))\n",
    "\n",
    "    return best_valid_rmse, best_model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b59cf1-eddb-4146-9830-3569b9fdde41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b51730f9c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alice looked at the jury-box, and saw that, in...</td>\n",
       "      <td>-0.432678</td>\n",
       "      <td>0.487498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4d403fd57</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Artificial_intel...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Artificial intelligence (AI) is intelligence e...</td>\n",
       "      <td>-1.161746</td>\n",
       "      <td>0.458396</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f789ee41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A gruff squire on horseback with shiny top boo...</td>\n",
       "      <td>-2.367914</td>\n",
       "      <td>0.519369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87f96eb79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>But that hadn't helped Washington.\\nThe Americ...</td>\n",
       "      <td>-0.842596</td>\n",
       "      <td>0.466193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b9cca6661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The principal business of the people of this c...</td>\n",
       "      <td>-0.748452</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                          url_legal       license  \\\n",
       "0  b51730f9c                                                NaN           NaN   \n",
       "1  4d403fd57  https://en.wikipedia.org/wiki/Artificial_intel...  CC BY-SA 3.0   \n",
       "2  0f789ee41                                                NaN           NaN   \n",
       "3  87f96eb79                                                NaN           NaN   \n",
       "4  b9cca6661                                                NaN           NaN   \n",
       "\n",
       "                                             excerpt    target  \\\n",
       "0  Alice looked at the jury-box, and saw that, in... -0.432678   \n",
       "1  Artificial intelligence (AI) is intelligence e... -1.161746   \n",
       "2  A gruff squire on horseback with shiny top boo... -2.367914   \n",
       "3  But that hadn't helped Washington.\\nThe Americ... -0.842596   \n",
       "4  The principal business of the people of this c... -0.748452   \n",
       "\n",
       "   standard_error  fold  \n",
       "0        0.487498     0  \n",
       "1        0.458396     2  \n",
       "2        0.519369     4  \n",
       "3        0.466193     0  \n",
       "4        0.433000     4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_oof = pd.read_csv(\"../../data/df_train_oof.csv\", index_col=0)  # 加载分层5折数据集\n",
    "data_oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d6b0587-0bfe-41af-8085-20e2029cb009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************0****************************************\n",
      "| end of epoch     1 | step:    10 | valild rmse  0.85086 |\n",
      "| end of epoch     1 | step:    19 | valild rmse  0.74202 |\n",
      "| end of epoch     1 | step:    46 | valild rmse  0.61767 |\n",
      "| end of epoch     1 | step:    69 | valild rmse  0.60545 |\n",
      "| end of epoch     1 | step:    90 | valild rmse  0.54625 |\n",
      "| end of epoch     1 | step:   180 | valild rmse  0.52896 |\n",
      "| end of epoch     1 | step:   197 | valild rmse  0.52730 |\n",
      "| end of epoch     1 | step:   211 | valild rmse  0.49932 |\n",
      "| end of epoch     1 | step:   258 | valild rmse  0.48944 |\n",
      "| end of epoch     2 | step:    32 | valild rmse  0.47923 |\n",
      "| end of epoch     2 | step:    88 | valild rmse  0.47525 |\n",
      "| end of epoch     2 | step:   161 | valild rmse  0.46992 |\n",
      "| end of epoch     3 | step:    19 | valild rmse  0.46981 |\n",
      "| end of epoch     3 | step:    29 | valild rmse  0.46832 |\n",
      "| end of epoch     3 | step:    60 | valild rmse  0.46192 |\n",
      "| end of epoch     3 | step:   270 | valild rmse  0.46128 |\n",
      "| end of epoch     4 | step:    72 | valild rmse  0.46046 |\n",
      "****************************************1****************************************\n",
      "| end of epoch     1 | step:    10 | valild rmse  0.91543 |\n",
      "| end of epoch     1 | step:    26 | valild rmse  0.77811 |\n",
      "| end of epoch     1 | step:    37 | valild rmse  0.65942 |\n",
      "| end of epoch     1 | step:    55 | valild rmse  0.57836 |\n",
      "| end of epoch     1 | step:    68 | valild rmse  0.55423 |\n",
      "| end of epoch     1 | step:   116 | valild rmse  0.54211 |\n",
      "| end of epoch     1 | step:   194 | valild rmse  0.52271 |\n",
      "| end of epoch     1 | step:   214 | valild rmse  0.52011 |\n",
      "| end of epoch     1 | step:   231 | valild rmse  0.49823 |\n",
      "| end of epoch     2 | step:     8 | valild rmse  0.49230 |\n",
      "| end of epoch     2 | step:    22 | valild rmse  0.47539 |\n",
      "| end of epoch     2 | step:   197 | valild rmse  0.47152 |\n",
      "| end of epoch     3 | step:    37 | valild rmse  0.46660 |\n",
      "| end of epoch     3 | step:    45 | valild rmse  0.46249 |\n",
      "| end of epoch     3 | step:   215 | valild rmse  0.46033 |\n",
      "| end of epoch     4 | step:    73 | valild rmse  0.45986 |\n",
      "| end of epoch     4 | step:   181 | valild rmse  0.45960 |\n",
      "| end of epoch     4 | step:   196 | valild rmse  0.45957 |\n",
      "| end of epoch     4 | step:   254 | valild rmse  0.45939 |\n",
      "| end of epoch     4 | step:   270 | valild rmse  0.45870 |\n",
      "****************************************2****************************************\n",
      "| end of epoch     1 | step:     5 | valild rmse  1.10760 |\n",
      "| end of epoch     1 | step:    18 | valild rmse  0.91388 |\n",
      "| end of epoch     1 | step:    38 | valild rmse  0.65640 |\n",
      "| end of epoch     1 | step:    61 | valild rmse  0.59541 |\n",
      "| end of epoch     1 | step:    69 | valild rmse  0.57573 |\n",
      "| end of epoch     1 | step:    97 | valild rmse  0.54812 |\n",
      "| end of epoch     1 | step:   106 | valild rmse  0.53693 |\n",
      "| end of epoch     1 | step:   194 | valild rmse  0.52883 |\n",
      "| end of epoch     1 | step:   214 | valild rmse  0.52335 |\n",
      "| end of epoch     1 | step:   267 | valild rmse  0.51055 |\n",
      "| end of epoch     2 | step:     7 | valild rmse  0.50122 |\n",
      "| end of epoch     2 | step:    54 | valild rmse  0.48289 |\n",
      "| end of epoch     2 | step:   150 | valild rmse  0.48194 |\n",
      "| end of epoch     3 | step:   101 | valild rmse  0.47840 |\n",
      "| end of epoch     3 | step:   146 | valild rmse  0.47661 |\n",
      "| end of epoch     3 | step:   183 | valild rmse  0.47517 |\n",
      "| end of epoch     3 | step:   191 | valild rmse  0.47461 |\n",
      "| end of epoch     4 | step:    64 | valild rmse  0.47431 |\n",
      "| end of epoch     4 | step:    81 | valild rmse  0.47323 |\n",
      "| end of epoch     4 | step:    98 | valild rmse  0.47288 |\n",
      "| end of epoch     4 | step:   113 | valild rmse  0.47126 |\n",
      "****************************************3****************************************\n",
      "| end of epoch     1 | step:     8 | valild rmse  1.04989 |\n",
      "| end of epoch     1 | step:    19 | valild rmse  0.89960 |\n",
      "| end of epoch     1 | step:    28 | valild rmse  0.80913 |\n",
      "| end of epoch     1 | step:    43 | valild rmse  0.69058 |\n",
      "| end of epoch     1 | step:    63 | valild rmse  0.60764 |\n",
      "| end of epoch     1 | step:    79 | valild rmse  0.58893 |\n",
      "| end of epoch     1 | step:    96 | valild rmse  0.56638 |\n",
      "| end of epoch     1 | step:   188 | valild rmse  0.56172 |\n",
      "| end of epoch     1 | step:   202 | valild rmse  0.53160 |\n",
      "| end of epoch     1 | step:   265 | valild rmse  0.51612 |\n",
      "| end of epoch     2 | step:    71 | valild rmse  0.49297 |\n",
      "| end of epoch     2 | step:   282 | valild rmse  0.49287 |\n",
      "| end of epoch     3 | step:     9 | valild rmse  0.48514 |\n",
      "| end of epoch     3 | step:    71 | valild rmse  0.47973 |\n",
      "| end of epoch     3 | step:   132 | valild rmse  0.47918 |\n",
      "| end of epoch     4 | step:   133 | valild rmse  0.47826 |\n",
      "| end of epoch     4 | step:   280 | valild rmse  0.47754 |\n",
      "| end of epoch     5 | step:    94 | valild rmse  0.47748 |\n",
      "****************************************4****************************************\n",
      "| end of epoch     1 | step:     6 | valild rmse  0.92765 |\n",
      "| end of epoch     1 | step:    19 | valild rmse  0.65843 |\n",
      "| end of epoch     1 | step:    31 | valild rmse  0.62754 |\n",
      "| end of epoch     1 | step:    41 | valild rmse  0.62013 |\n",
      "| end of epoch     1 | step:    56 | valild rmse  0.61995 |\n",
      "| end of epoch     1 | step:    72 | valild rmse  0.61491 |\n",
      "| end of epoch     1 | step:    91 | valild rmse  0.56923 |\n",
      "| end of epoch     1 | step:   182 | valild rmse  0.53417 |\n",
      "| end of epoch     1 | step:   216 | valild rmse  0.52560 |\n",
      "| end of epoch     1 | step:   277 | valild rmse  0.52276 |\n",
      "| end of epoch     2 | step:    62 | valild rmse  0.50204 |\n",
      "| end of epoch     2 | step:   134 | valild rmse  0.49879 |\n",
      "| end of epoch     2 | step:   244 | valild rmse  0.48066 |\n",
      "| end of epoch     3 | step:    36 | valild rmse  0.47115 |\n",
      "| end of epoch     3 | step:   205 | valild rmse  0.46894 |\n",
      "| end of epoch     3 | step:   213 | valild rmse  0.46854 |\n",
      "| end of epoch     3 | step:   231 | valild rmse  0.46798 |\n",
      "| end of epoch     4 | step:    94 | valild rmse  0.46618 |\n"
     ]
    }
   ],
   "source": [
    "fold_num_list = range(5)\n",
    "\n",
    "best_valid_rmse_lst = []  # 每折交叉验证最佳模型验证数据集的rmse\n",
    "best_model_state_dict_lst = []  # 每折交叉验证最佳模型的状态字典\n",
    "\n",
    "for fold in fold_num_list:\n",
    "    print('*' * 40 + str(fold) + '*' * 40)\n",
    "\n",
    "    train_data = data_oof[data_oof['fold'] != fold]  # 训练数据集\n",
    "    val_data = data_oof[data_oof['fold'] == fold]  # 验证数据集\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset=MyDataset(train_data), batch_size=8,\n",
    "                                                   collate_fn=get_collate_fn(tokenizer), shuffle=True)\n",
    "    dataloader_val = torch.utils.data.DataLoader(dataset=MyDataset(val_data),\n",
    "                                                 # 测试数据只有7条((\\sqrt{(n1 + n2)/2} + \\sqrt{(n3 + n4)/2} + \\sqrt{(n5 + n6)/2}) / 3 不等于\\sqrt{(n1 + n2 + n3 + n4 + n5 + n6)/6})\n",
    "                                                 batch_size=7,\n",
    "                                                 collate_fn=get_collate_fn(tokenizer),\n",
    "                                                 shuffle=False, drop_last=True)\n",
    "\n",
    "    # **************************************************************************************\n",
    "    electra_large = MyModel(copy.deepcopy(pretrained))  # 必须进行深拷贝(pretrained会参与更新),否则会造成标签泄露\n",
    "    electra_large = electra_large.to(device)\n",
    "\n",
    "    loss_mse = nn.MSELoss()\n",
    "\n",
    "    parameters = get_parameters(electra_large, 2e-5, 0.95, 1e-4)\n",
    "    # 优化器\n",
    "    optimizer_adamw = optim.AdamW(parameters)\n",
    "    scheduler_lr = get_cosine_schedule_with_warmup(optimizer_adamw, 0, len(dataloader_train) * 5)\n",
    "    # **************************************************************************************\n",
    "\n",
    "    bvr, bmsd = train_and_evaluate(electra_large, dataloader_train, dataloader_val, loss_mse, optimizer_adamw,\n",
    "                                   scheduler_lr, 5, device)\n",
    "\n",
    "    best_valid_rmse_lst.append(bvr)\n",
    "    best_model_state_dict_lst.extend(bmsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7743b0b9-ba2a-43f6-9880-0bb413abeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    predict_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in dataloader:\n",
    "            # 数据设备\n",
    "            input_ids = i[0].to(device)\n",
    "            attention_mask = i[1].to(device)\n",
    "            token_type_ids = i[2].to(device)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            predict_list.append(out.cpu())\n",
    "    predict_all = torch.cat(predict_list, dim=0)\n",
    "    return predict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc8154a9-de3e-4f9e-8ab4-f0694985c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tr_predict = torch.tensor([]).reshape((0, 1))\n",
    "all_te_predict = torch.tensor([]).reshape((0, 1))\n",
    "\n",
    "for fold in fold_num_list:\n",
    "    val_d = data_oof[data_oof['fold'] == fold]\n",
    "    dl_val = torch.utils.data.DataLoader(dataset=MyDataset(val_d), batch_size=8, collate_fn=get_collate_fn(tokenizer))\n",
    "    # **************************************************************************************\n",
    "    electra_large = MyModel(copy.deepcopy(pretrained))  # 必须进行深拷贝(pretrained会参与更新),否则会造成标签泄露\n",
    "    electra_large.load_state_dict(best_model_state_dict_lst[fold])  # 记载状态字典\n",
    "    electra_large = electra_large.to(device)\n",
    "    # **************************************************************************************\n",
    "    predict_result_tr = predict(electra_large, dl_val, device)\n",
    "    all_tr_predict = torch.cat([all_tr_predict, predict_result_tr])\n",
    "\n",
    "    predict_result_te = predict(electra_large, dataloader_test, device)\n",
    "    all_te_predict = torch.cat([all_te_predict, predict_result_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f6d6f6-b336-431a-b019-8466d9b89593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2834, 1]), torch.Size([35, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tr_predict.shape, all_te_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24f3475e-ad93-4a0c-acab-6769c4a529b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['electra_large_tr.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(all_tr_predict.numpy(), 'electra_large_tr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0452e8b7-1b58-4639-81fa-7db8ab2aa174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['electra_large_te.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(all_te_predict.numpy(), 'electra_large_te.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f2451-bf1e-4d8a-9f8f-32122f536848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}